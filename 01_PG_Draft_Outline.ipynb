{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- This is a summary program based on Langchain's short course 'Chat with your data'\n",
    "- A simplified version that uses PDF file only to show how a document is transformed\n",
    "    - Load multiple PDF files\n",
    "    - Split loaded files into smaller chunks\n",
    "    - Chunks are embedded using OpenAI embedding\n",
    "    - Store these embeddings in a Vector database (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My secret key is: 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.\n"
     ]
    }
   ],
   "source": [
    "# Basic setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import openai\n",
    "\n",
    "from dotenv import  load_dotenv\n",
    "\n",
    "load_dotenv() # read local .env file\n",
    "\n",
    "# openai_api_key='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'\n",
    "\n",
    "openai.api_key=os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "# openai.api_key  = os.environ[OPENAI_API_KEY]\n",
    "\n",
    "def myEnvironment():\n",
    "    # print(f'My id is: {my_id}.')\n",
    "    print(f'My secret key is: {openai.api_key}.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myEnvironment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langchain provides a <loader> module for loading both structured and unstructured data sources. Here we are using PyPDF loader for uploading sample PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "PyPDFLoader(\"docs1/MachineLearning-Lecture01.pdf\"),\n",
    "PyPDFLoader(\"docs1/MachineLearning-Lecture02.pdf\"),\n",
    "PyPDFLoader(\"docs1/MachineLearning-Lecture03.pdf\")\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find out how many pages have been loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Break uploaded PDFs into smaller chunks. We need to define, chunk size, chunk overlap. Langchain provides RecursiveCharacterTextSplitter and Character text splitter. Major difference is \n",
    "    - Recursive character text splitter - is preferred for Generic text splitting\n",
    "    - Character text splitter splits on new line character. If you set the character on space it will be split as recursive CTS\n",
    "    - double new line separater between paragraphs\n",
    "    - Recursive Character text splitters has other separators as well such /n/n, /n, \" \", \"\" by default\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "chunk_size = 120\n",
    "chunk_overlap = 10\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "\n",
    "splits = r_splitter.split_documents(\n",
    "\n",
    "docs\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore these chucks\n",
    "    - How many chunks?\n",
    "    - What is inside them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1889"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the number of chunks\n",
    "\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine', metadata={'source': 'docs1/MachineLearning-Lecture01.pdf', 'page': 0}),\n",
       " Document(page_content='learning class. So what I wanna do today is ju st spend a little time going over the logistics', metadata={'source': 'docs1/MachineLearning-Lecture01.pdf', 'page': 0}),\n",
       " Document(page_content=\"of the class, and then we'll start to  talk a bit about machine learning.\", metadata={'source': 'docs1/MachineLearning-Lecture01.pdf', 'page': 0}),\n",
       " Document(page_content=\"By way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so\", metadata={'source': 'docs1/MachineLearning-Lecture01.pdf', 'page': 0}),\n",
       " Document(page_content=\"I personally work in machine learning, and I' ve worked on it for about 15 years now, and\", metadata={'source': 'docs1/MachineLearning-Lecture01.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the text of a chunk\n",
    "\n",
    "splits[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand Embeddings\n",
    "    - Use OpenAI embedding\n",
    "    - Understand similarity between words and sentences\n",
    "    - Use `dot` product to compare embeddings, higher score is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_1 = \"Apple\",\n",
    "embedding_2 = \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0028599445902576914, -0.005424751648017928, -0.01598874169942126, -0.021673343066195377, -0.021006076492248848, 0.027409271484722164, -0.007866050426361114, -0.005344551267009154, -0.003952273323249115, -0.0062492115089087695]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_2[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.dot(embedding_1, embedding_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487800690436125"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding_1, embedding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7966507280236204"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding_2, embedding_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7636250274290804"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding_1, embedding_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.870826933839829"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding_5, embedding_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup a vector database to store all splits after embedding them\n",
    "    - Run `pip install chromadb`\n",
    "    - Need to create a directory where all splits will be stored and pass it as persist_path\n",
    "    - Need to pass embedding information, we are using OpenAI embeddings\n",
    "    - All splits as document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is another attempt of using Chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding function\n",
    "# did not work as pip install sentence-transformers was not installed due to some space issue in the server\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# new_client = chromadb.EphemeralClient()\n",
    "openai_lc_client = Chroma.from_documents(\n",
    "    documents=splits, embedding=embeddings)\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "docs = openai_lc_client.similarity_search(query, k=3)\n",
    "len(docs)\n",
    "# print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory='docs1/chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_lc_client.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents = splits,\n",
    "    persist_directory=persist_directory,\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievar = openai_lc_client.as_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a branch of artificial intelligence that focuses on developing algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. It involves the use of statistical techniques and pattern recognition to train computers to learn and improve from experience.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is machine learning?\"\n",
    "\n",
    "result = qa_chain(query)\n",
    "\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(splits, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count all embeddings in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9445\n"
     ]
    }
   ],
   "source": [
    "print(openai_lc_client._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using file metadata for better contextulization. Add a filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understanding embedding serach\n",
    "    - Similarity search\n",
    "    - Max marginal relevance search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"add some sample question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vectordb.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `k` is used to define number of docs to be returned, let us check the count of returned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the content of the returned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Persist the database for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "- Retrieval using similarity search\n",
    "- Max Marginal Query for similarity and introducing diversity in the returned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrieval import similarity_serach\n",
    "\n",
    "ss = similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding context of page metadat and using self retrieval query using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'VectorStoreRetriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/langchain-chat/01_PG_Draft_Outline.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c616e67636861696e2d524147227d/home/ubuntu/langchain-chat/01_PG_Draft_Outline.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m qa_chain \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39;49mfrom_chain_type(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c616e67636861696e2d524147227d/home/ubuntu/langchain-chat/01_PG_Draft_Outline.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     llm,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c616e67636861696e2d524147227d/home/ubuntu/langchain-chat/01_PG_Draft_Outline.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     retrievar\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c616e67636861696e2d524147227d/home/ubuntu/langchain-chat/01_PG_Draft_Outline.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:100\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m _chain_type_kwargs \u001b[39m=\u001b[39m chain_type_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 100\u001b[0m combine_documents_chain \u001b[39m=\u001b[39m load_qa_chain(\n\u001b[1;32m    101\u001b[0m     llm, chain_type\u001b[39m=\u001b[39;49mchain_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_chain_type_kwargs\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(combine_documents_chain\u001b[39m=\u001b[39mcombine_documents_chain, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py:244\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load question answering chain.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m    A chain to use for question answering.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m loader_mapping: Mapping[\u001b[39mstr\u001b[39m, LoadingCallable] \u001b[39m=\u001b[39m {\n\u001b[1;32m    239\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m: _load_stuff_chain,\n\u001b[1;32m    240\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmap_reduce\u001b[39m\u001b[39m\"\u001b[39m: _load_map_reduce_chain,\n\u001b[1;32m    241\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrefine\u001b[39m\u001b[39m\"\u001b[39m: _load_refine_chain,\n\u001b[1;32m    242\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmap_rerank\u001b[39m\u001b[39m\"\u001b[39m: _load_map_rerank_chain,\n\u001b[1;32m    243\u001b[0m }\n\u001b[0;32m--> 244\u001b[0m \u001b[39mif\u001b[39;00m chain_type \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unsupported chain type: \u001b[39m\u001b[39m{\u001b[39;00mchain_type\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould be one of \u001b[39m\u001b[39m{\u001b[39;00mloader_mapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m loader_mapping[chain_type](\n\u001b[1;32m    250\u001b[0m     llm, verbose\u001b[39m=\u001b[39mverbose, callback_manager\u001b[39m=\u001b[39mcallback_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    251\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'VectorStoreRetriever'"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retrievar = openai_lc_client.as_re\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
